{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Dataset-exploration\" data-toc-modified-id=\"Dataset-exploration-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Content-of-the-files\" data-toc-modified-id=\"Content-of-the-files-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Content of the files</a></span></li><li><span><a href=\"#Importing-the-data\" data-toc-modified-id=\"Importing-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Importing the data</a></span></li></ul></li><li><span><a href=\"#Detecting-similar-books\" data-toc-modified-id=\"Detecting-similar-books-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Detecting similar books</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-titles-to-obtain-candidate-similars\" data-toc-modified-id=\"Using-titles-to-obtain-candidate-similars-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Using titles to obtain candidate similars</a></span></li><li><span><a href=\"#Enriching-our-dataset-using-Amazon-product-Advertising-API\" data-toc-modified-id=\"Enriching-our-dataset-using-Amazon-product-Advertising-API-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Enriching our dataset using Amazon product Advertising API</a></span></li><li><span><a href=\"#Using-author-list-to-refine-candidate-similars\" data-toc-modified-id=\"Using-author-list-to-refine-candidate-similars-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Using author list to refine candidate similars</a></span></li><li><span><a href=\"#Using-titles-to-obtain-our-final-set-of-similar-books\" data-toc-modified-id=\"Using-titles-to-obtain-our-final-set-of-similar-books-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Using titles to obtain our final set of similar books</a></span></li><li><span><a href=\"#Saving-the-data-using-pickle-for-further-analysis\" data-toc-modified-id=\"Saving-the-data-using-pickle-for-further-analysis-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Saving the data using pickle for further analysis</a></span></li></ul></li><li><span><a href=\"#Correlation-analysis\" data-toc-modified-id=\"Correlation-analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Correlation analysis</a></span></li><li><span><a href=\"#Herding-Effect-analysis\" data-toc-modified-id=\"Herding-Effect-analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Herding Effect analysis</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# To reload external scripts automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Importing external files\n",
    "from scripts.similarities import *\n",
    "from scripts.amazon_api_interaction import *\n",
    "from scripts.analysis import *\n",
    "from scripts.data_import import *\n",
    "from scripts.utils_project import *\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn', Mutes warnings when copying a slice from a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have tried to depict the full pipeline that we used for this project. It will not mention all the research that was done initially on a smaller dataset but those can be found in the ```research.ipynb```. Also note that most of the methods we use have been externalized to python files that can be found in ```scripts```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "_The goal of this project is to study the effect of the first review on an amazon product to subsequent reviews. This is also called the Herding effect._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started we wanted to explain our strategy. We have decided to focus on Amazon books as those make a great product to study the Herding effect as they are often available in different edition which do not modify the content.\n",
    "1. The most challenging part was to **Identify similar books**. Indeed with a very large number of books it was complex to use pair wise comparison to do such task. Therefore we used Locality sensitive Hashing on the titles of the books (you can refer to the section for further explanations).\n",
    "    * This method yields bins of candidate similar books that needs further refinements. While we first wanted to use the description of the book to find for example authors and other information, it turns out that most description are simply comments from journalists on the book and is therefore not that helpful. We decided to use **Amazon product advertising API** to obtain more metadata on the books that we found to be candidate similars.\n",
    "    * Then **we used the authors of each book to refine candidate similars** where we tried to match the list of authors between two candidates by matching each author in a list to the author in the second book that had the least Levenstein distance. Once this matching was realized we look at the mean distance (the levenstein distance was transformed into a relative value to allow the comparison) and we consider valid two books whose mean relative levenstein distance is lower than 0.35. This phase allowed to delete books that had similar titles but not at all similar authors.\n",
    "    * Finally we cleaned the titles from any unnecessary characters (parenthesis, colon, all lower case etc.) and decided to match two books if and only if the **symmetric difference of their normalized titles** is empty (which means that one of the set is entirely contained in the other).\n",
    "2. Once we had those similar books, the real analysis can start. First of all we wanted to study the impact of reviews on the way user shop on the website (by using several metrics, trying for example to see the impact of average rating on the sales rank)\n",
    "3. The very last step consisted in showing the Herding effect:\n",
    "    * **Pre-analysis** of the obtained dataset was necessary in order to categorize the books into 6 categories (```HH,HM,HL,MM,ML,LL```) where ```H``` means that the first review of a book is High, ```M``` means that it is medium and ```L``` it is low. (you can refer to the section itself to see how we mapped those categories to the number of stars a book had)\n",
    "    * We then plotted the average ratings for each groups depending on the fact that the book belong to the first or second letter of its category in order to see the **influence of the first review on the following**. Because of a quick convergence toward a similar value, we only analyzed the effect on the 5 first reviews.\n",
    "    * As a final step we wanted to show the review average for each group on a **long term basis**. That is we wanted to see if on average the same book that had a first ```L``` rating has a lower review average in the long term than when it had a ```H``` first rating (comparing ```H``` in ```HL``` and ```L``` in ```HL```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../../Project-Data/\"\n",
    "META_FOLDER = DATA_FOLDER + \"meta/\"\n",
    "REVIEWS_FOLDER = DATA_FOLDER + \"reviews/\"\n",
    "CORE_FOLDER = DATA_FOLDER + \"5_core/\"\n",
    "DUMP_FOLDER = DATA_FOLDER + \"dump/\"\n",
    "CATEGORIES = ['Books','Movies_and_Tv','Electronics']\n",
    "MAXCOUNT = -1\n",
    "ANALYSIS_DATA_FOLDER = 'analysis_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have a lot of data we decided to first look at the content of the dataset in order to import into dataframes only the metadata that can be useful for our study. Note that we have three different types of file: **metadata**, **reviews** and **5-core** (which contains only reviews on products and reviewers that have at least 5 reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different columns of the **metadata** files are : \n",
    "* ```asin``` : the unique identifier of the object\n",
    "* ```brand```\n",
    "* ```categories``` : the categories of the object\n",
    "* ```description``` : the description of the object\n",
    "* ```imUrl```  : the link toward the images related to the object\n",
    "* ```price```\n",
    "* ```related``` : a list of objects that are related to this object\n",
    "* ```salesRank``` \n",
    "* ```title```\n",
    "\n",
    "The different columns of the **reviews** and **5-core** files are :\n",
    "* ```asin``` : the unique identifier of the object\n",
    "* ```helpful``` : a list of 2 integers [x,y], the helpfulness score is x/y votes\n",
    "* ```overall``` : the rating of the object\n",
    "* ```reviewText```\n",
    "* ```reviewTime```\n",
    "* ```reviewerID```\n",
    "* ```reviewerName```\n",
    "* ```summary ``` : the title of the review\n",
    "* ```unixReviewTime``` : in Unix format\n",
    "\n",
    "Therefore we keep only the column that are of interest for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_interesting_cols = ['asin', 'title', 'salesRank', 'description','imUrl']\n",
    "review_interesting_cols = ['asin', 'overall', 'unixReviewTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with large JSON would add a layer of complexity and therefore we imported the files to dataframes. A full pipeline has been made to do so. (NB: even-though the 5 core data was used for the exploratory phase we do not use it in the final study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths : \n",
      "\t meta = ../../Project-Data/meta/meta_Books.json\n",
      "\t review = ../../Project-Data/reviews/reviews_Books.json\n",
      "\t core_path = ../../Project-Data/5_core/Books.json\n",
      "Retrieving from : ../../Project-Data/dump/meta_Books_asin_title_salesRank_description_imUrl_ALL\n",
      "It took 00:00:05.051 to import the data.\n",
      "Retrieving from : ../../Project-Data/dump/reviews_Books_asin_overall_unixReviewTime_ALL\n",
      "It took 00:00:02.071 to import the data.\n"
     ]
    }
   ],
   "source": [
    "meta_books_path, review_books_path, core_book_path = get_paths(\n",
    "    0, DATA_FOLDER, META_FOLDER, CORE_FOLDER, REVIEWS_FOLDER, CATEGORIES)\n",
    "meta_books = import_interesting_cols(\n",
    "    meta_books_path,\n",
    "    DUMP_FOLDER,\n",
    "    True,\n",
    "    meta_interesting_cols,\n",
    "    max_count=MAXCOUNT)\n",
    "review_books = import_interesting_cols(\n",
    "    review_books_path,\n",
    "    DUMP_FOLDER,\n",
    "    False,\n",
    "    review_interesting_cols,\n",
    "    max_count=MAXCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meta_books dataframe has 2370585 books and 21 attributes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>description</th>\n",
       "      <th>imUrl</th>\n",
       "      <th>salesRank_Arts,_Crafts_&amp;_Sewing</th>\n",
       "      <th>salesRank_Books</th>\n",
       "      <th>salesRank_Cell_Phones_&amp;_Accessories</th>\n",
       "      <th>salesRank_Clothing</th>\n",
       "      <th>salesRank_Electronics</th>\n",
       "      <th>salesRank_Health_&amp;_Personal_Care</th>\n",
       "      <th>salesRank_Home_&amp;_Kitchen</th>\n",
       "      <th>...</th>\n",
       "      <th>salesRank_Jewelry</th>\n",
       "      <th>salesRank_Kitchen_&amp;_Dining</th>\n",
       "      <th>salesRank_Movies_&amp;_TV</th>\n",
       "      <th>salesRank_Music</th>\n",
       "      <th>salesRank_Musical_Instruments</th>\n",
       "      <th>salesRank_Office_Products</th>\n",
       "      <th>salesRank_Shoes</th>\n",
       "      <th>salesRank_Sports_&amp;_Outdoors</th>\n",
       "      <th>salesRank_Toys_&amp;_Games</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001048791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51MKP0T4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6334800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Crucible: Performed by Stuart Pankin, Jero...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin description                                              imUrl  \\\n",
       "0  0001048791         NaN  http://ecx.images-amazon.com/images/I/51MKP0T4...   \n",
       "\n",
       "   salesRank_Arts,_Crafts_&_Sewing  salesRank_Books  \\\n",
       "0                              NaN        6334800.0   \n",
       "\n",
       "   salesRank_Cell_Phones_&_Accessories  salesRank_Clothing  \\\n",
       "0                                  NaN                 NaN   \n",
       "\n",
       "   salesRank_Electronics  salesRank_Health_&_Personal_Care  \\\n",
       "0                    NaN                               NaN   \n",
       "\n",
       "   salesRank_Home_&_Kitchen  \\\n",
       "0                       NaN   \n",
       "\n",
       "                         ...                          salesRank_Jewelry  \\\n",
       "0                        ...                                        NaN   \n",
       "\n",
       "   salesRank_Kitchen_&_Dining  salesRank_Movies_&_TV  salesRank_Music  \\\n",
       "0                         NaN                    NaN              NaN   \n",
       "\n",
       "   salesRank_Musical_Instruments  salesRank_Office_Products  salesRank_Shoes  \\\n",
       "0                            NaN                        NaN              NaN   \n",
       "\n",
       "   salesRank_Sports_&_Outdoors  salesRank_Toys_&_Games  \\\n",
       "0                          NaN                     NaN   \n",
       "\n",
       "                                               title  \n",
       "0  The Crucible: Performed by Stuart Pankin, Jero...  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The meta_books dataframe has {} books and {} attributes\".format(meta_books.shape[0],meta_books.shape[1]))\n",
    "meta_books.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review_books dataframe has 22507155 books and 3 attributes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000000116</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-04-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  overall unixReviewTime\n",
       "0  0000000116      4.0     2002-04-27"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The review_books dataframe has {} reviews and {} attributes\".format(review_books.shape[0],review_books.shape[1]))\n",
    "review_books.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting similar books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now start to detect similar books. We have in previous exploratory discarded reviewers with less than 5 reviews and books with less than 5 reviews. While this latter does make sense for the Herding Effect analysis, the restriction on reviewers doesn't make much sense. We will therefore still keep the restriction on books of having at least 5 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the full dataset we have 2370585 books and  22507155 reviews\n",
      "We found 625682 books with enough reviews.\n"
     ]
    }
   ],
   "source": [
    "print(\"In the full dataset we have {} books and  {} reviews\".format(meta_books.shape[0],review_books.shape[0]))\n",
    "books_reviews_count = review_books.groupby('asin').count().overall\n",
    "asin_enough_reviews = list(books_reviews_count[books_reviews_count > 5].index)\n",
    "print(\"We found {} books with enough reviews.\".format(len(asin_enough_reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained the identifiers of the books with enough reviews we will keep only those in the ```meta_books``` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the books dataset is composed of 625682 books\n"
     ]
    }
   ],
   "source": [
    "meta_books_reduced = meta_books[meta_books['asin'].isin(asin_enough_reviews)]\n",
    "print(\"Now the books dataset is composed of {} books\".format(meta_books_reduced.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working on large datasets we took the habit to serialize our intermediate work to reduce the running time of the code. We will serialize those in the ```../../Project-Data/dump_full/``` folder (you can refer to the ```README``` to observe the file architecture of the project). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DUMP_FOLDER_FULL = DATA_FOLDER + 'dump_full/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using titles to obtain candidate similars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LSH (local sensitivity hashing) we will now try to find bins of candidate similars using the tiltes of the amazon products. Therefore we will keep in our dataframe only the books with an existing title and only the columns that can later be of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>imUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0000913154</td>\n",
       "      <td>The Way Things Work: An Illustrated Encycloped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/7113akhD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0001055178</td>\n",
       "      <td>Master Georgie</td>\n",
       "      <td>Beryl Bainbridge seems drawn to disaster. Firs...</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51ZSC6TK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0001018043</td>\n",
       "      <td>The Enchanted Horse</td>\n",
       "      <td>Grade 3-6-In this fast-paced fantasy, a neglec...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0001473727</td>\n",
       "      <td>The Greatest Book on \"Dispensational Truth\" in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/512M299K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0000230022</td>\n",
       "      <td>The Simple Truths of Service: Inspired by John...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/218uMkP0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          asin                                              title  \\\n",
       "10  0000913154  The Way Things Work: An Illustrated Encycloped...   \n",
       "13  0001055178                                     Master Georgie   \n",
       "25  0001018043                                The Enchanted Horse   \n",
       "34  0001473727  The Greatest Book on \"Dispensational Truth\" in...   \n",
       "37  0000230022  The Simple Truths of Service: Inspired by John...   \n",
       "\n",
       "                                          description  \\\n",
       "10                                                NaN   \n",
       "13  Beryl Bainbridge seems drawn to disaster. Firs...   \n",
       "25  Grade 3-6-In this fast-paced fantasy, a neglec...   \n",
       "34                                                NaN   \n",
       "37                                                NaN   \n",
       "\n",
       "                                                imUrl  \n",
       "10  http://ecx.images-amazon.com/images/I/7113akhD...  \n",
       "13  http://ecx.images-amazon.com/images/I/51ZSC6TK...  \n",
       "25                                                NaN  \n",
       "34  http://ecx.images-amazon.com/images/I/512M299K...  \n",
       "37  http://ecx.images-amazon.com/images/I/218uMkP0...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_books_reduced  = meta_books_reduced [meta_books_reduced ['title'].notnull()]\n",
    "meta_books_reduced = meta_books_reduced[['asin','title','description','imUrl']]\n",
    "meta_books_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving from : ../../Project-Data/dump_full/candidate_dup_title_2_100_5_4\n",
      "Found 15449 bins of possible duplicates.\n",
      "With 36066 different books\n"
     ]
    }
   ],
   "source": [
    "dump_path = DUMP_FOLDER_FULL\n",
    "get_candidate_full = candidate_duplicates(meta_books_reduced,dump_path,['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NB: the hashing used involves random number and therefore different run of the method ```candidate_duplicates``` might result in slightly different bins. This is one of the reason why we also serialize the candidates in order to always have the same data to deduct conclusion in further work*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching our dataset using Amazon product Advertising API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because amazon API has strong limitations on the number of requests per account and the number of queries per seconds, we try to use it as efficiently as possible:\n",
    "* we decided to limit our QPS (query per second) to 0.9 as the limit is 1 QPS. \n",
    "* whenever the API throttles our queries we wait a random amount of time before retrying (following a random variable with exponential distribution of parameter 0.1, as advised by Amazon).\n",
    "* each query retrieves 10 books' details at a time.\n",
    "\n",
    "This will allow us to get many more details of our books : ```authors, publisher, ISBN, sales_rank_updated, binding, edition, release_date```. We might not need all of them but because of the restriction of the API we decided to obtain as much as possible for each book should we decide to use it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving from : \n",
      "\t../../Project-Data/dump_full/book_only_candidates_with_details\n",
      "\t../../Project-Data/dump_full/api_failed \n",
      "It took 00:00:00.181 to get the data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>imUrl</th>\n",
       "      <th>authors</th>\n",
       "      <th>publisher</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>sales_rank_updated</th>\n",
       "      <th>binding</th>\n",
       "      <th>edition</th>\n",
       "      <th>release_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001473727</td>\n",
       "      <td>The Greatest Book on \"Dispensational Truth\" in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/512M299K...</td>\n",
       "      <td>[C. Larkin]</td>\n",
       "      <td>Titles distributed by Christian Art Distributors</td>\n",
       "      <td>0001473727</td>\n",
       "      <td>837246</td>\n",
       "      <td>Relié</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001472933</td>\n",
       "      <td>The Book of Daniel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/414KH1FP...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Rev Clarence Larkin Estate</td>\n",
       "      <td>0001472933</td>\n",
       "      <td>1569511</td>\n",
       "      <td>Relié</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                              title description  \\\n",
       "0  0001473727  The Greatest Book on \"Dispensational Truth\" in...         NaN   \n",
       "1  0001472933                                 The Book of Daniel         NaN   \n",
       "\n",
       "                                               imUrl      authors  \\\n",
       "0  http://ecx.images-amazon.com/images/I/512M299K...  [C. Larkin]   \n",
       "1  http://ecx.images-amazon.com/images/I/414KH1FP...           []   \n",
       "\n",
       "                                          publisher        ISBN  \\\n",
       "0  Titles distributed by Christian Art Distributors  0001473727   \n",
       "1                        Rev Clarence Larkin Estate  0001472933   \n",
       "\n",
       "  sales_rank_updated binding edition release_date  \n",
       "0             837246   Relié    None         None  \n",
       "1            1569511   Relié    None         None  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_access_file_path = DUMP_FOLDER + \"amazon_access_epfl_new\"\n",
    "book_only_candidates,failed = fill_in_with_details(meta_books_reduced,amazon_access_file_path,get_candidate_full,DUMP_FOLDER_FULL)\n",
    "book_only_candidates.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some book ASIN might not have been successfully retrieved for numerous reasons, one of which might be that the object isn't sold anymore. We saved such failed ASIN in the ```failed``` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_only_candidates.shape =  (36066, 11)\n",
      "550 have failed\n"
     ]
    }
   ],
   "source": [
    "print(\"book_only_candidates.shape = \",book_only_candidates.shape)\n",
    "print(\"{} have failed\".format(len(failed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using author list to refine candidate similars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the matching will now use authors of each books, we will eliminate from our dataset the books for which no authors was found (in the case where the API couldn't retrieve the details of the book using the ASIN) but also in the case where it is an empty list (the API call was successful but no author was returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This leaves us with 33337 books to work with\n"
     ]
    }
   ],
   "source": [
    "book_only_candidates = book_only_candidates[book_only_candidates['authors'].notnull()]\n",
    "book_only_candidates = book_only_candidates[book_only_candidates['authors'].apply(lambda x:len(x)>0)]\n",
    "print(\"This leaves us with {} books to work with\".format(book_only_candidates.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we also need to keep in our dictionnary ```get_candidate_full```, obtained from LSH, only the books that have an author list that allow us to match them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 13913 bins of similar books to work with\n"
     ]
    }
   ],
   "source": [
    "compliant_asins = set(book_only_candidates['asin'])\n",
    "similar_sets = [set(get_candidate_full[k]+[k]) for k in get_candidate_full.keys()]\n",
    "similar_sets_compliant=[]\n",
    "for s in similar_sets:\n",
    "    inter = s & compliant_asins\n",
    "    if(len(inter)>1):\n",
    "        similar_sets_compliant.append(inter)\n",
    "print(\"We now have {} bins of similar books to work with\".format(len(similar_sets_compliant)))\n",
    "\n",
    "# And we put it back into a dictionnary\n",
    "similars = [sorted(list(s)) for s in similar_sets_compliant]\n",
    "get_candidate = {elements[0]:elements[1:] for elements in similars}\n",
    "book_only_candidates = book_only_candidates.set_index('asin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw examples where the authors list between two books were very similar but names were written differently, therefore we need to check that some typos are not in the names of authors : (e.g. ```{'Ian MacNeill', 'SportMed BC'}{'Ian MacNeil'}``` where MacNeil is written with two 'l' in the first book but only one in the second one). Therefore we will first clean the names (remove accents, nomalize it to lower case) and then try to match then using the Levenstein distance, we match two author list if the mean difference between authors is less that 0.35 (NB: this is a relative distance in order to be able to compare title with different length). \n",
    "\n",
    "See the function ```check_name_similarity``` in the ```utils.py``` files for more details on how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 5987 bins of candidate similars\n"
     ]
    }
   ],
   "source": [
    "book_only_candidates['authors'] = book_only_candidates['authors'].apply(clean_name_in_dataframe)\n",
    "similars = get_similar_authors(book_only_candidates,get_candidate,0.35)\n",
    "print(\"We have found {} bins of candidate similars\".format(len(similars.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have some unwanted behaviour of the author matching function. Indeed because we want to avoid at all cost false positives we have some matching that could trivially be done by humans : 'sir arthur conan doyle' and 'a conan doyle' are considered but should be considered similar. We decided to leave the task of refining the matching to a later study and possibly to a reader that would like to extend our work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using titles to obtain our final set of similar books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we might still have problems with books that have highly similar titles and authors but aren't identical. This could be explained by :\n",
    "* Different Tomes : same title but different number\n",
    "* educational books that have identical titles except for what they teach\n",
    "* simply typos or very small differences \n",
    "* many other examples \n",
    "\n",
    "Example of books that are not similar:\n",
    "```\n",
    "Adobe Dreamweaver CS3 Classroom in a Book['adobe creative team']\n",
    "Adobe Dreamweaver CS4 Classroom in a Book['adobe creative team']\n",
    "Adobe Dreamweaver CS5 Classroom in a Book['adobe creative team']\n",
    "Adobe Dreamweaver CS6 Classroom in a Book['adobe creative team']\n",
    "----------------------------------------------------------------\n",
    "Promethea, Book 3['alan moore']\n",
    "Promethea, Book 5['alan moore']\n",
    "Promethea, Book 1['alan moore', 'j. h. williams', 'mick gray']\n",
    "\n",
    "```\n",
    "\n",
    "Example of books that should be considered similar:\n",
    "``` \n",
    "'Light Science and Magic: An Introduction to Photographic Lighting'['fil hunter', 'paul fuqua', 'steven biver']\n",
    "'Light: Science and Magic: An Introduction to Photographic Lighting'['fil hunter', 'steven biver', 'paul fuqua']\n",
    "```\n",
    "\n",
    "We will therefore also need to see if the title are similar enough. **We want to avoid false positive as much as possible. We are sure that we could go even further with matching names but prefer to be conservative here**. Therefore we only try to normalize as much as possible the titles and see if then the titles taken as sets have an empty symmetric difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 3050 bins of very similar books.\n",
      "We have 6410 asins for products that have at least one similar product.\n"
     ]
    }
   ],
   "source": [
    "book_only_candidates['title'] = book_only_candidates['title'].apply(clean_title)\n",
    "very_similars = get_similar_titles(book_only_candidates,similars)\n",
    "print(\"We have found {} bins of very similar books.\".format(len(very_similars.keys())))\n",
    "very_similar_ASIN = list(very_similars.keys())\n",
    "very_similar_ASIN += [item for sublist in list(very_similars.values()) for item in sublist]\n",
    "print(\"We have {} asins for products that have at least one similar product.\".format(len(very_similar_ASIN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ```observational_print``` argument of ```get_similar_titles``` one can observe how our function is quite conservative and why:\n",
    "\n",
    "An example where the titles were not matched, providing there difference:\n",
    "```\n",
    "'college mathematics for business economics life sciences and social sciences 12th edition barnett'\n",
    "'finite mathematics for business economics life sciences and social sciences 12th edition barnett'\n",
    "\t --->'college','finite'\n",
    "```\n",
    "and another one\n",
    "```\n",
    "'mastering the nikon d700'\n",
    "'mastering the nikon d600'\n",
    "\t --->'d700','d600\n",
    "```\n",
    "\n",
    "We can see what titles are going to be matched : \n",
    "*What is interesting here is that the last title contains a repetition of the title inside parenthesis, after the cleaning, we will have removed the parenthesis and will check if the symmetric difference in terms of set of words is empty.*\n",
    "```\n",
    "\"Jeff Herman's Guide to Book Publishers, Editors & Literary Agents: Who They Are! What They Want! and How to Win Them Over!\",\n",
    "\"Jeff Herman's Guide to Book Publishers, Editors and Literary Agents : Who They Are! What They Want! How to Win Them Over!\",\n",
    "\"Jeff Herman's Guide to Book Publishers, Editors, and Literary Agents: Who They Are! What They Want! How to Win Them Over! (Jeff Herman's Guide to Book Publishers, Editors, & Literary Agents)\"\n",
    "```\n",
    "*or this second example where a ':' has been replace with a ';'*\n",
    "```\n",
    "'Hawaii the Big Island Revealed: The Ultimate Guidebook',\n",
    "'Hawaii The Big Island Revealed; The Ultimate Guidebook',\n",
    "```\n",
    "\n",
    "\n",
    "While we know that we might be missing a lot of titles, we much rather have less title and 0 false positives than to risk having false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data using pickle for further analysis\n",
    "Now that we have obtained the dataset that we wish to use to conduct our analysis it is time to save it locally to avoid long and repetitive operations while we do the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ANALYSIS_DATA_FOLDER = 'analysis_data/'\n",
    "books_df_path = ANALYSIS_DATA_FOLDER + 'books_full'\n",
    "review_books_df_path = ANALYSIS_DATA_FOLDER + 'review_books_full'\n",
    "very_similars_path = ANALYSIS_DATA_FOLDER + 'very_similars_full'\n",
    "\n",
    "paths = [books_df_path,review_books_df_path,very_similars_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving from :\n",
      "\tanalysis_data/books_full\n",
      "\tanalysis_data/review_books_full\n",
      "\tanalysis_data/very_similars_full\n",
      "We have retrieved:\n",
      "\tbooks_df with shape: (6410, 10)\n",
      "\treview_books_df with shape: (361389, 3)\n",
      "\tvery_similars with length: 3050\n"
     ]
    }
   ],
   "source": [
    "retrieved = []\n",
    "# Get the file from the dump\n",
    "if(all(os.path.isfile(p) for p in paths)):\n",
    "    # We check if the files already exist\n",
    "    print(\"Retrieving from :\")\n",
    "    for i,p in enumerate(paths):\n",
    "        print(\"\\t{}\".format(p))\n",
    "        if(i<2):\n",
    "            df = pd.read_pickle(p)\n",
    "            retrieved.append(df)\n",
    "        else:\n",
    "            with open(p, 'rb') as handle:\n",
    "                very_similars = pickle.load(handle)\n",
    "                retrieved.append(very_similars)\n",
    "else:\n",
    "    # Otherwise we create them\n",
    "    book_only_candidates.loc[very_similar_ASIN].to_pickle(ANALYSIS_DATA_FOLDER + 'books_full')\n",
    "    review_books.query('asin in @very_similar_ASIN').to_pickle(ANALYSIS_DATA_FOLDER + 'review_books_full')\n",
    "    with open(ANALYSIS_DATA_FOLDER + 'very_similars_full', 'wb') as handle:\n",
    "        pickle.dump(very_similars, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "books_df,review_books_df,very_similars=retrieved\n",
    "print(\"We have retrieved:\\n\\tbooks_df with shape: {}\\n\\treview_books_df with shape: {}\\n\\tvery_similars with length: {}\".format(books_df.shape,review_books_df.shape,len(very_similars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herding Effect analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
