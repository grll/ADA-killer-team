{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Dataset-exploration\" data-toc-modified-id=\"Dataset-exploration-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Content-of-the-files\" data-toc-modified-id=\"Content-of-the-files-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Content of the files</a></span></li><li><span><a href=\"#Importing-the-data\" data-toc-modified-id=\"Importing-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Importing the data</a></span></li></ul></li><li><span><a href=\"#Detecting-similar-books\" data-toc-modified-id=\"Detecting-similar-books-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Detecting similar books</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-titles-to-obtain-candidate-similars\" data-toc-modified-id=\"Using-titles-to-obtain-candidate-similars-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Using titles to obtain candidate similars</a></span><ul class=\"toc-item\"><li><span><a href=\"#Local-Sensitivity-Hashing\" data-toc-modified-id=\"Local-Sensitivity-Hashing-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Local Sensitivity Hashing</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-minhash\" data-toc-modified-id=\"The-minhash-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>The <code>minhash</code></a></span></li><li><span><a href=\"#The-principle-of-LSH\" data-toc-modified-id=\"The-principle-of-LSH-3.1.1.2\"><span class=\"toc-item-num\">3.1.1.2&nbsp;&nbsp;</span>The principle of LSH</a></span></li></ul></li><li><span><a href=\"#Using-LSH-to-work-on-titles\" data-toc-modified-id=\"Using-LSH-to-work-on-titles-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Using LSH to work on titles</a></span></li><li><span><a href=\"#Inspecting-the-result-of-LSH-on-books-titles\" data-toc-modified-id=\"Inspecting-the-result-of-LSH-on-books-titles-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Inspecting the result of LSH on books titles</a></span></li></ul></li><li><span><a href=\"#Enriching-our-dataset-using-Amazon-product-Advertising-API\" data-toc-modified-id=\"Enriching-our-dataset-using-Amazon-product-Advertising-API-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Enriching our dataset using Amazon product Advertising API</a></span></li><li><span><a href=\"#Using-author-list-to-refine-candidate-similars\" data-toc-modified-id=\"Using-author-list-to-refine-candidate-similars-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Using author list to refine candidate similars</a></span></li><li><span><a href=\"#Using-titles-to-obtain-our-final-set-of-similar-books\" data-toc-modified-id=\"Using-titles-to-obtain-our-final-set-of-similar-books-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Using titles to obtain our final set of similar books</a></span></li><li><span><a href=\"#Saving-the-data-using-pickle-for-further-analysis\" data-toc-modified-id=\"Saving-the-data-using-pickle-for-further-analysis-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Saving the data using pickle for further analysis</a></span></li></ul></li><li><span><a href=\"#Correlation-analysis\" data-toc-modified-id=\"Correlation-analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Correlation analysis</a></span></li><li><span><a href=\"#Herding-Effect-analysis\" data-toc-modified-id=\"Herding-Effect-analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Herding Effect analysis</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# To reload external scripts automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Importing external files\n",
    "from scripts.similarities import *\n",
    "from scripts.amazon_api_interaction import *\n",
    "from scripts.analysis import *\n",
    "from scripts.data_import import *\n",
    "from scripts.utils_project import *\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn', Mutes warnings when copying a slice from a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have tried to depict the full pipeline that we used for this project. It will not mention all the research that was done initially on a smaller dataset but those can be found in the ```research.ipynb```. Also note that most of the methods we use have been externalized to python files that can be found in ```scripts```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "_The goal of this project is to study the effect of the first review on an amazon product to subsequent reviews. This is also called the Herding effect._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started we wanted to explain our strategy. We have decided to focus on Amazon books as those make a great product to study the Herding effect as they are often available in different edition which do not modify the content.\n",
    "1. The most challenging part was to **Identify similar books**. Indeed with a very large number of books it was complex to use pair wise comparison to do such task. Therefore we used Locality sensitive Hashing on the titles of the books (you can refer to the section for further explanations).\n",
    "    * This method yields bins of candidate similar books that needs further refinements. While we first wanted to use the description of the book to find for example authors and other information, it turns out that most description are simply comments from journalists on the book and is therefore not that helpful. We decided to use **Amazon product advertising API** to obtain more metadata on the books that we found to be candidate similars.\n",
    "    * Then **we used the authors of each book to refine candidate similars** where we tried to match the list of authors between two candidates by matching each author in a list to the author in the second book that had the least Levenstein distance. Once this matching was realized we look at the mean distance (the levenstein distance was transformed into a relative value to allow the comparison) and we consider valid two books whose mean relative levenstein distance is lower than 0.35. This phase allowed to delete books that had similar titles but not at all similar authors.\n",
    "    * Finally we cleaned the titles from any unnecessary characters (parenthesis, colon, all lower case etc.) and decided to match two books if and only if the **symmetric difference of their normalized titles** is empty (which means that one of the set is entirely contained in the other).\n",
    "2. Once we had those similar books, the real analysis can start. First of all we wanted to study the impact of reviews on the way user shop on the website (by using several metrics, trying for example to see the impact of average rating on the sales rank)\n",
    "3. The very last step consisted in showing the Herding effect:\n",
    "    * **Pre-analysis** of the obtained dataset was necessary in order to categorize the books into 6 categories (```HH,HM,HL,MM,ML,LL```) where ```H``` means that the first review of a book is High, ```M``` means that it is medium and ```L``` it is low. (you can refer to the section itself to see how we mapped those categories to the number of stars a book had)\n",
    "    * We then plotted the average ratings for each groups depending on the fact that the book belong to the first or second letter of its category in order to see the **influence of the first review on the following**. Because of a quick convergence toward a similar value, we only analyzed the effect on the 5 first reviews.\n",
    "    * As a final step we wanted to show the review average for each group on a **long term basis**. That is we wanted to see if on average the same book that had a first ```L``` rating has a lower review average in the long term than when it had a ```H``` first rating (comparing ```H``` in ```HL``` and ```L``` in ```HL```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../../Project-Data/\"\n",
    "META_FOLDER = DATA_FOLDER + \"meta/\"\n",
    "REVIEWS_FOLDER = DATA_FOLDER + \"reviews/\"\n",
    "CORE_FOLDER = DATA_FOLDER + \"5_core/\"\n",
    "DUMP_FOLDER = DATA_FOLDER + \"dump/\"\n",
    "CATEGORIES = ['Books','Movies_and_Tv','Electronics']\n",
    "MAXCOUNT = -1\n",
    "ANALYSIS_DATA_FOLDER = 'analysis_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have a lot of data we decided to first look at the content of the dataset in order to import into dataframes only the metadata that can be useful for our study. Note that we have three different types of file: **metadata**, **reviews** and **5-core** (which contains only reviews on products and reviewers that have at least 5 reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different columns of the **metadata** files are : \n",
    "* ```asin``` : the unique identifier of the object\n",
    "* ```brand```\n",
    "* ```categories``` : the categories of the object\n",
    "* ```description``` : the description of the object\n",
    "* ```imUrl```  : the link toward the images related to the object\n",
    "* ```price```\n",
    "* ```related``` : a list of objects that are related to this object\n",
    "* ```salesRank``` \n",
    "* ```title```\n",
    "\n",
    "The different columns of the **reviews** and **5-core** files are :\n",
    "* ```asin``` : the unique identifier of the object\n",
    "* ```helpful``` : a list of 2 integers [x,y], the helpfulness score is x/y votes\n",
    "* ```overall``` : the rating of the object\n",
    "* ```reviewText```\n",
    "* ```reviewTime```\n",
    "* ```reviewerID```\n",
    "* ```reviewerName```\n",
    "* ```summary ``` : the title of the review\n",
    "* ```unixReviewTime``` : in Unix format\n",
    "\n",
    "Therefore we keep only the column that are of interest for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_interesting_cols = ['asin', 'title', 'salesRank', 'description','imUrl']\n",
    "review_interesting_cols = ['asin', 'overall', 'unixReviewTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with large JSON would add a layer of complexity and therefore we imported the files to dataframes. A full pipeline has been made to do so. (NB: even-though the 5 core data was used for the exploratory phase we do not use it in the final study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths : \n",
      "\t meta = ../../Project-Data/meta/meta_Books.json\n",
      "\t review = ../../Project-Data/reviews/reviews_Books.json\n",
      "\t core_path = ../../Project-Data/5_core/Books.json\n",
      "Retrieving from : ../../Project-Data/dump/meta_Books_asin_title_salesRank_description_imUrl_ALL\n",
      "It took 00:00:05.051 to import the data.\n",
      "Retrieving from : ../../Project-Data/dump/reviews_Books_asin_overall_unixReviewTime_ALL\n",
      "It took 00:00:02.071 to import the data.\n"
     ]
    }
   ],
   "source": [
    "meta_books_path, review_books_path, core_book_path = get_paths(\n",
    "    0, DATA_FOLDER, META_FOLDER, CORE_FOLDER, REVIEWS_FOLDER, CATEGORIES)\n",
    "meta_books = import_interesting_cols(\n",
    "    meta_books_path,\n",
    "    DUMP_FOLDER,\n",
    "    True,\n",
    "    meta_interesting_cols,\n",
    "    max_count=MAXCOUNT)\n",
    "review_books = import_interesting_cols(\n",
    "    review_books_path,\n",
    "    DUMP_FOLDER,\n",
    "    False,\n",
    "    review_interesting_cols,\n",
    "    max_count=MAXCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meta_books dataframe has 2370585 books and 21 attributes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>description</th>\n",
       "      <th>imUrl</th>\n",
       "      <th>salesRank_Arts,_Crafts_&amp;_Sewing</th>\n",
       "      <th>salesRank_Books</th>\n",
       "      <th>salesRank_Cell_Phones_&amp;_Accessories</th>\n",
       "      <th>salesRank_Clothing</th>\n",
       "      <th>salesRank_Electronics</th>\n",
       "      <th>salesRank_Health_&amp;_Personal_Care</th>\n",
       "      <th>salesRank_Home_&amp;_Kitchen</th>\n",
       "      <th>...</th>\n",
       "      <th>salesRank_Jewelry</th>\n",
       "      <th>salesRank_Kitchen_&amp;_Dining</th>\n",
       "      <th>salesRank_Movies_&amp;_TV</th>\n",
       "      <th>salesRank_Music</th>\n",
       "      <th>salesRank_Musical_Instruments</th>\n",
       "      <th>salesRank_Office_Products</th>\n",
       "      <th>salesRank_Shoes</th>\n",
       "      <th>salesRank_Sports_&amp;_Outdoors</th>\n",
       "      <th>salesRank_Toys_&amp;_Games</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001048791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51MKP0T4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6334800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Crucible: Performed by Stuart Pankin, Jero...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin description                                              imUrl  \\\n",
       "0  0001048791         NaN  http://ecx.images-amazon.com/images/I/51MKP0T4...   \n",
       "\n",
       "   salesRank_Arts,_Crafts_&_Sewing  salesRank_Books  \\\n",
       "0                              NaN        6334800.0   \n",
       "\n",
       "   salesRank_Cell_Phones_&_Accessories  salesRank_Clothing  \\\n",
       "0                                  NaN                 NaN   \n",
       "\n",
       "   salesRank_Electronics  salesRank_Health_&_Personal_Care  \\\n",
       "0                    NaN                               NaN   \n",
       "\n",
       "   salesRank_Home_&_Kitchen  \\\n",
       "0                       NaN   \n",
       "\n",
       "                         ...                          salesRank_Jewelry  \\\n",
       "0                        ...                                        NaN   \n",
       "\n",
       "   salesRank_Kitchen_&_Dining  salesRank_Movies_&_TV  salesRank_Music  \\\n",
       "0                         NaN                    NaN              NaN   \n",
       "\n",
       "   salesRank_Musical_Instruments  salesRank_Office_Products  salesRank_Shoes  \\\n",
       "0                            NaN                        NaN              NaN   \n",
       "\n",
       "   salesRank_Sports_&_Outdoors  salesRank_Toys_&_Games  \\\n",
       "0                          NaN                     NaN   \n",
       "\n",
       "                                               title  \n",
       "0  The Crucible: Performed by Stuart Pankin, Jero...  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The meta_books dataframe has {} books and {} attributes\".format(meta_books.shape[0],meta_books.shape[1]))\n",
    "meta_books.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review_books dataframe has 22507155 books and 3 attributes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000000116</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-04-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  overall unixReviewTime\n",
       "0  0000000116      4.0     2002-04-27"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The review_books dataframe has {} reviews and {} attributes\".format(review_books.shape[0],review_books.shape[1]))\n",
    "review_books.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting similar books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now start to detect similar books. We have in previous exploratory discarded reviewers with less than 5 reviews and books with less than 5 reviews. While this latter does make sense for the Herding Effect analysis, the restriction on reviewers doesn't make much sense. We will therefore still keep the restriction on books of having at least 5 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the full dataset we have 2370585 books and  22507155 reviews\n",
      "We found 625682 books with enough reviews.\n"
     ]
    }
   ],
   "source": [
    "print(\"In the full dataset we have {} books and  {} reviews\".format(meta_books.shape[0],review_books.shape[0]))\n",
    "books_reviews_count = review_books.groupby('asin').count().overall\n",
    "asin_enough_reviews = list(books_reviews_count[books_reviews_count > 5].index)\n",
    "print(\"We found {} books with enough reviews.\".format(len(asin_enough_reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained the identifiers of the books with enough reviews we will keep only those in the ```meta_books``` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the books dataset is composed of 625682 books\n"
     ]
    }
   ],
   "source": [
    "meta_books_reduced = meta_books[meta_books['asin'].isin(asin_enough_reviews)]\n",
    "print(\"Now the books dataset is composed of {} books\".format(meta_books_reduced.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working on large datasets we took the habit to serialize our intermediate work to reduce the running time of the code. We will serialize those in the ```../../Project-Data/dump_full/``` folder (you can refer to the ```README``` to observe the file architecture of the project). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DUMP_FOLDER_FULL = DATA_FOLDER + 'dump_full/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using titles to obtain candidate similars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explain quickly how LSH works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Local Sensitivity Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on this [article](http://dataconomy.com/2017/06/locality-sensitive-hashing-pydata/), and following the theory that can be read at the chapter 3.4 of [Mining of massive datasets by Leskovec, Rajaraman and Milliway](http://infolab.stanford.edu/~ullman/mmds/book.pdf) we will try to find similar books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### The ```minhash```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We need to detect near similar books in this large datasets. To do so we would need to compare each of the attribute of a given book with all the others which is extremely computationaly expensive. There we will use LSH. Right now the title and description of the book have variable length depending on the books. To perform LSH we would like to have a fixed length representation of the document without modifying the semantics of document similarity.\n",
    "* first we introduce the principle of shingles. A shingle of 5-gram (hence we discard the last shingle as it only consists in the $<5$ last characters of the document) e.g. is a set of all possible 5-grams in the string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4666666666666667"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"Lorem Ipsum dolor sit amet\"\n",
    "shingles = get_shingles(document)\n",
    "other_document = 'Lorem Ipsum dolor sit amet with some extra garbage'\n",
    "other_shingles = get_shingles(other_document)\n",
    "jaccard_dist(shingles,other_shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each document will still have a different number of shingles depending on its length. Therefore we wish to represent it using a *fixed length representation*. We will present now a function ```minhash``` that has a collision probability that is exactly the jaccard similarity (based on this [document](http://infolab.stanford.edu/~ullman/mmds/ch3.pdf) and following this [repo](https://github.com/mattilyra/LSH/blob/master/examples/Introduction.ipynb)) . We explain this now :\n",
    "1. Suppose that the following dataframe is the boolean variable corresponding to whether or not a shingle belongs to a document\n",
    "2. the ```minhash``` of a document returns a random permutation of the rows and then the first row number it founds where the value is non 0.\n",
    "    * therefore row 1 for doc 1\n",
    "    * and row 0 for doc 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>shingleID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc1  doc2  shingleID\n",
       "0     0     1          1\n",
       "1     1     1          2\n",
       "2     0     1          3\n",
       "3     0     0          4\n",
       "4     1     1          5\n",
       "5     1     0          6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_minhash = pd.DataFrame({'shingleID':[1,2,3,4,5,6],'doc1':[0,1,0,0,1,1],'doc2':[1,1,1,0,1,0]})\n",
    "ex_minhash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But this is for this particular permutation, if we had another one like below then the ```minhash``` would return :\n",
    "   * row 2 for doc1\n",
    "   * row 1 for doc2\n",
    "\n",
    "There are a lot of such permutations. Nevertheless we only care about having the same row for each document which mean we have collision of the ```minhash```. Therefore we have only two rows for which this happens : $\\text{shingleID} \\in \\{2,5\\}$. Hence the probability that the two doc have the same ```minhash``` (remmember that rows with two zeros do not count as they are not considered by ```minhash```) is the number of rows where both doc value is 1 divided by the number of rows where they are different : $\\frac{2}{5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>shingleID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc1  doc2  shingleID\n",
       "0     0     1          1\n",
       "3     0     0          4\n",
       "4     1     1          5\n",
       "2     0     1          3\n",
       "5     1     0          6\n",
       "1     1     1          2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_minhash.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can chose the length of the fingerprint that is returned by the ```minhash```, as we increase the length we get less variance from the random initialisation of the ```minhasher``` but this also greatly increases the memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### The principle of LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lsh import minhash\n",
    "hasher = minhash.MinHasher(seeds=100, char_ngram=5)\n",
    "fingerprint0 = hasher.fingerprint('Lorem Ipsum dolor sit amet'.encode('utf8'))\n",
    "len(fingerprint0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Therefore the hash of a document is composed of a given (```seeds```) number of ```minhashes```. We divide this number of ```minhashes``` into a given number of parts (e.g. 5). Since every single ```minhash``` has the jaccard similarity probability of collision then each of the 5 parts will have this probability as well. These parts represent the **locality** in the term LSH.\n",
    "\n",
    "Then we hash the content of each part using a different hash function to obtain the *binID* that represents the **hashing** part of the method. Into each bin with *binID* we store the entire fingerprint of the document.\n",
    "\n",
    "The idea is then that we compare documents that fall in the same bins : we will compare their fingerprint which is equivalent to looking at their Jaccard similarity between shingle sets. Since not all documents will fall into the same bins we have reduced the number of potential candidates.\n",
    "\n",
    "Indeed we call :\n",
    "* ```seeds``` :  number of ```minihashes``` that compose the fingerprint\n",
    "* ```bands``` : the number of bins that we want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LSH to work on titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LSH (local sensitivity hashing) we will now try to find bins of candidate similars using the tiltes of the amazon products. Therefore we will keep in our dataframe only the books with an existing title and only the columns that can later be of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>imUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0000913154</td>\n",
       "      <td>The Way Things Work: An Illustrated Encycloped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/7113akhD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0001055178</td>\n",
       "      <td>Master Georgie</td>\n",
       "      <td>Beryl Bainbridge seems drawn to disaster. Firs...</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51ZSC6TK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0001018043</td>\n",
       "      <td>The Enchanted Horse</td>\n",
       "      <td>Grade 3-6-In this fast-paced fantasy, a neglec...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0001473727</td>\n",
       "      <td>The Greatest Book on \"Dispensational Truth\" in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/512M299K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0000230022</td>\n",
       "      <td>The Simple Truths of Service: Inspired by John...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/218uMkP0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          asin                                              title  \\\n",
       "10  0000913154  The Way Things Work: An Illustrated Encycloped...   \n",
       "13  0001055178                                     Master Georgie   \n",
       "25  0001018043                                The Enchanted Horse   \n",
       "34  0001473727  The Greatest Book on \"Dispensational Truth\" in...   \n",
       "37  0000230022  The Simple Truths of Service: Inspired by John...   \n",
       "\n",
       "                                          description  \\\n",
       "10                                                NaN   \n",
       "13  Beryl Bainbridge seems drawn to disaster. Firs...   \n",
       "25  Grade 3-6-In this fast-paced fantasy, a neglec...   \n",
       "34                                                NaN   \n",
       "37                                                NaN   \n",
       "\n",
       "                                                imUrl  \n",
       "10  http://ecx.images-amazon.com/images/I/7113akhD...  \n",
       "13  http://ecx.images-amazon.com/images/I/51ZSC6TK...  \n",
       "25                                                NaN  \n",
       "34  http://ecx.images-amazon.com/images/I/512M299K...  \n",
       "37  http://ecx.images-amazon.com/images/I/218uMkP0...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_books_reduced  = meta_books_reduced [meta_books_reduced ['title'].notnull()]\n",
    "meta_books_reduced = meta_books_reduced[['asin','title','description','imUrl']]\n",
    "meta_books_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving from : ../../Project-Data/dump_full/candidate_dup_title_2_100_5_4\n",
      "Found 15449 bins of possible duplicates.\n",
      "With 36066 different books\n"
     ]
    }
   ],
   "source": [
    "dump_path = DUMP_FOLDER_FULL\n",
    "get_candidate_full = candidate_duplicates(meta_books_reduced,dump_path,['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NB: the hashing used involves random number and therefore different run of the method ```candidate_duplicates``` might result in slightly different bins. This is one of the reason why we also serialize the candidates in order to always have the same data to deduct conclusion in further work*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>imUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0000913154</td>\n",
       "      <td>The Way Things Work: An Illustrated Encycloped...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/7113akhD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0001055178</td>\n",
       "      <td>Master Georgie</td>\n",
       "      <td>Beryl Bainbridge seems drawn to disaster. Firs...</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51ZSC6TK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0001018043</td>\n",
       "      <td>The Enchanted Horse</td>\n",
       "      <td>Grade 3-6-In this fast-paced fantasy, a neglec...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0001473727</td>\n",
       "      <td>The Greatest Book on \"Dispensational Truth\" in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/512M299K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0000230022</td>\n",
       "      <td>The Simple Truths of Service: Inspired by John...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/218uMkP0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          asin                                              title  \\\n",
       "10  0000913154  The Way Things Work: An Illustrated Encycloped...   \n",
       "13  0001055178                                     Master Georgie   \n",
       "25  0001018043                                The Enchanted Horse   \n",
       "34  0001473727  The Greatest Book on \"Dispensational Truth\" in...   \n",
       "37  0000230022  The Simple Truths of Service: Inspired by John...   \n",
       "\n",
       "                                          description  \\\n",
       "10                                                NaN   \n",
       "13  Beryl Bainbridge seems drawn to disaster. Firs...   \n",
       "25  Grade 3-6-In this fast-paced fantasy, a neglec...   \n",
       "34                                                NaN   \n",
       "37                                                NaN   \n",
       "\n",
       "                                                imUrl  \n",
       "10  http://ecx.images-amazon.com/images/I/7113akhD...  \n",
       "13  http://ecx.images-amazon.com/images/I/51ZSC6TK...  \n",
       "25                                                NaN  \n",
       "34  http://ecx.images-amazon.com/images/I/512M299K...  \n",
       "37  http://ecx.images-amazon.com/images/I/218uMkP0...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_books_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Inspecting the result of LSH on books titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dreamer\n",
      "------\n",
      "Dreamer\n",
      "------\n",
      "Dreamer\n",
      "------\n",
      "At the center of National Book Award winner Charles Johnson's novelDreamerare three remarkable men: Martin Luther King Jr.; his aide, Matthew  Bishop, an African American philosophy student; and Chaym Smith, a man who is a dead  ringer for the civil rights leader. Not only does Smith resemble King, but he also shares  his intellectual voracity, widely read in both Eastern and Western philosophy, proficient  in Sanskrit and martial arts, and a talented painter. But where King is deeply spiritual,  Smith is a cynic; where King has the full force of his strong beliefs and his strong family  heritage, Smith has nothing but a lifetime of misfortune to shape his attitudes. When he  offers to become King's stand-in, Johnson creates an ideal situation in which to explore  issues long at the heart of the \"race issue\" in America: the inequality between black and  white, even between black and black.As the novel moves forward in time toward that fateful day in Memphis, Johnson  concentrates on the relationship between Bishop--the narrator--and Smith, a man who,  with better luck, might have been as great as King. Periodically, the author also lets us in  on King's own meditations on his life and faith, and the movement to which he has given  them. All in all,Dreameris the kind of novel Charles Johnson does so well: a  book about a big subject, chock full of ideas and populated by characters articulate  enough to argue them.\n",
      "------\n",
      "Phillip L. Davidson is an attorney who lives in Nashville, Tennessee with his wife, Karen. He is a former infantry Captain who commanded a group of Cambodian and Vietnamese Kit Carson Scouts on a night ambush in the Mekong Delta. He is currently at work on a second novel.\n",
      "------\n",
      "\"This offbeat first novel of psychological horror keeps the reader alert with its sweeping, fast-paced transformations and canny fantasies of desire, success and inexplicable loss.\" -Publishers Weekly\"Horror at the End of the Century: A Reading List.\" Included wereDreamer, Thomas Harris'sTheSilence of the Lambs, Peter Straub'sMrs. God, Doris Lessing'sThe Fifth Child, Dan Simmons'sChildren of the Night, and 57 others.  -The New York Review of Science Fiction\"A good, convoluted first novel about a man who is either being fiendishly manipulated or is crazy.\" (Mentioned alongside such \"noteworthy novels\" as Doris Lessing'sThe Fifth Child,Peter Straub'sKoko, Thomas Harris'sThe Silence of the Lambs, and Anne Rice'sQueen of the Damned.) -- Ellen Datlow, The Year's Best Fantasy Second Annual Collection\n",
      "------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://ecx.images-amazon.com/images/I/5114C9WA1TL.jpg\" width=\"250\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://ecx.images-amazon.com/images/I/71HAfTJA9cL.jpg\" width=\"250\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://ecx.images-amazon.com/images/I/41b9MUBM1aL.jpg\" width=\"250\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's quickly oberve the type of data that we obtain\n",
    "element_to_inspect = 66\n",
    "similarity_keys = list(get_candidate_full.keys())\n",
    "for t in get_titles(similarity_keys[element_to_inspect],get_candidate_full,meta_books_reduced):\n",
    "    print(t)\n",
    "    print(\"------\")\n",
    "for d in get_desc(similarity_keys[element_to_inspect],get_candidate_full,meta_books_reduced):\n",
    "    print(d)\n",
    "    print(\"------\")\n",
    "display_images(similarity_keys[element_to_inspect],get_candidate_full,meta_books_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can see that the books can greatly vary even though they have the same title. Also it seems hard to extract any meaningful data from their description therefore we need to use the Amazon API to try to get the data we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching our dataset using Amazon product Advertising API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because amazon API has strong limitations on the number of requests per account and the number of queries per seconds, we try to use it as efficiently as possible:\n",
    "* we decided to limit our QPS (query per second) to 0.9 as the limit is 1 QPS. \n",
    "* whenever the API throttles our queries we wait a random amount of time before retrying (following a random variable with exponential distribution of parameter 0.1, as advised by Amazon).\n",
    "* each query retrieves 10 books' details at a time.\n",
    "\n",
    "This will allow us to get many more details of our books : ```authors, publisher, ISBN, sales_rank_updated, binding, edition, release_date```. We might not need all of them but because of the restriction of the API we decided to obtain as much as possible for each book should we decide to use it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving from : \n",
      "\t../../Project-Data/dump_full/book_only_candidates_with_details\n",
      "\t../../Project-Data/dump_full/api_failed \n",
      "It took 00:00:00.181 to get the data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>imUrl</th>\n",
       "      <th>authors</th>\n",
       "      <th>publisher</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>sales_rank_updated</th>\n",
       "      <th>binding</th>\n",
       "      <th>edition</th>\n",
       "      <th>release_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001473727</td>\n",
       "      <td>The Greatest Book on \"Dispensational Truth\" in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/512M299K...</td>\n",
       "      <td>[C. Larkin]</td>\n",
       "      <td>Titles distributed by Christian Art Distributors</td>\n",
       "      <td>0001473727</td>\n",
       "      <td>837246</td>\n",
       "      <td>Relié</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001472933</td>\n",
       "      <td>The Book of Daniel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/414KH1FP...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Rev Clarence Larkin Estate</td>\n",
       "      <td>0001472933</td>\n",
       "      <td>1569511</td>\n",
       "      <td>Relié</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                              title description  \\\n",
       "0  0001473727  The Greatest Book on \"Dispensational Truth\" in...         NaN   \n",
       "1  0001472933                                 The Book of Daniel         NaN   \n",
       "\n",
       "                                               imUrl      authors  \\\n",
       "0  http://ecx.images-amazon.com/images/I/512M299K...  [C. Larkin]   \n",
       "1  http://ecx.images-amazon.com/images/I/414KH1FP...           []   \n",
       "\n",
       "                                          publisher        ISBN  \\\n",
       "0  Titles distributed by Christian Art Distributors  0001473727   \n",
       "1                        Rev Clarence Larkin Estate  0001472933   \n",
       "\n",
       "  sales_rank_updated binding edition release_date  \n",
       "0             837246   Relié    None         None  \n",
       "1            1569511   Relié    None         None  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_access_file_path = DUMP_FOLDER + \"amazon_access_epfl_new\"\n",
    "book_only_candidates,failed = fill_in_with_details(meta_books_reduced,amazon_access_file_path,get_candidate_full,DUMP_FOLDER_FULL)\n",
    "book_only_candidates.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some book ASIN might not have been successfully retrieved for numerous reasons, one of which might be that the object isn't sold anymore. We saved such failed ASIN in the ```failed``` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_only_candidates.shape =  (36066, 11)\n",
      "550 have failed\n"
     ]
    }
   ],
   "source": [
    "print(\"book_only_candidates.shape = \",book_only_candidates.shape)\n",
    "print(\"{} have failed\".format(len(failed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using author list to refine candidate similars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the matching will now use authors of each books, we will eliminate from our dataset the books for which no authors was found (in the case where the API couldn't retrieve the details of the book using the ASIN) but also in the case where it is an empty list (the API call was successful but no author was returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This leaves us with 33337 books to work with\n"
     ]
    }
   ],
   "source": [
    "book_only_candidates = book_only_candidates[book_only_candidates['authors'].notnull()]\n",
    "book_only_candidates = book_only_candidates[book_only_candidates['authors'].apply(lambda x:len(x)>0)]\n",
    "print(\"This leaves us with {} books to work with\".format(book_only_candidates.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we also need to keep in our dictionnary ```get_candidate_full```, obtained from LSH, only the books that have an author list that allow us to match them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 13913 bins of similar books to work with\n"
     ]
    }
   ],
   "source": [
    "compliant_asins = set(book_only_candidates['asin'])\n",
    "similar_sets = [set(get_candidate_full[k]+[k]) for k in get_candidate_full.keys()]\n",
    "similar_sets_compliant=[]\n",
    "for s in similar_sets:\n",
    "    inter = s & compliant_asins\n",
    "    if(len(inter)>1):\n",
    "        similar_sets_compliant.append(inter)\n",
    "print(\"We now have {} bins of similar books to work with\".format(len(similar_sets_compliant)))\n",
    "\n",
    "# And we put it back into a dictionnary\n",
    "similars = [sorted(list(s)) for s in similar_sets_compliant]\n",
    "get_candidate = {elements[0]:elements[1:] for elements in similars}\n",
    "book_only_candidates = book_only_candidates.set_index('asin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw examples where the authors list between two books were very similar but names were written differently, therefore we need to check that some typos are not in the names of authors : (e.g. ```{'Ian MacNeill', 'SportMed BC'}{'Ian MacNeil'}``` where MacNeil is written with two 'l' in the first book but only one in the second one). Therefore we will first clean the names (remove accents, nomalize it to lower case) and then try to match then using the Levenstein distance, we match two author list if the mean difference between authors is less that 0.35 (NB: this is a relative distance in order to be able to compare title with different length). \n",
    "\n",
    "See the function ```check_name_similarity``` in the ```utils.py``` files for more details on how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 5987 bins of candidate similars\n"
     ]
    }
   ],
   "source": [
    "book_only_candidates['authors'] = book_only_candidates['authors'].apply(clean_name_in_dataframe)\n",
    "similars = get_similar_authors(book_only_candidates,get_candidate,0.35)\n",
    "print(\"We have found {} bins of candidate similars\".format(len(similars.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have some unwanted behaviour of the author matching function. Indeed because we want to avoid at all cost false positives we have some matching that could trivially be done by humans : 'sir arthur conan doyle' and 'a conan doyle' are considered but should be considered similar. We decided to leave the task of refining the matching to a later study and possibly to a reader that would like to extend our work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using titles to obtain our final set of similar books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we might still have problems with books that have highly similar titles and authors but aren't identical. This could be explained by :\n",
    "* Different Tomes : same title but different number\n",
    "* educational books that have identical titles except for what they teach\n",
    "* simply typos or very small differences \n",
    "* many other examples \n",
    "\n",
    "Example of books that are not similar:\n",
    "```\n",
    "Adobe Dreamweaver CS3 Classroom in a Book['adobe creative team']\n",
    "Adobe Dreamweaver CS4 Classroom in a Book['adobe creative team']\n",
    "Adobe Dreamweaver CS5 Classroom in a Book['adobe creative team']\n",
    "Adobe Dreamweaver CS6 Classroom in a Book['adobe creative team']\n",
    "----------------------------------------------------------------\n",
    "Promethea, Book 3['alan moore']\n",
    "Promethea, Book 5['alan moore']\n",
    "Promethea, Book 1['alan moore', 'j. h. williams', 'mick gray']\n",
    "\n",
    "```\n",
    "\n",
    "Example of books that should be considered similar:\n",
    "``` \n",
    "'Light Science and Magic: An Introduction to Photographic Lighting'['fil hunter', 'paul fuqua', 'steven biver']\n",
    "'Light: Science and Magic: An Introduction to Photographic Lighting'['fil hunter', 'steven biver', 'paul fuqua']\n",
    "```\n",
    "\n",
    "We will therefore also need to see if the title are similar enough. **We want to avoid false positive as much as possible. We are sure that we could go even further with matching names but prefer to be conservative here**. Therefore we only try to normalize as much as possible the titles and see if then the titles taken as sets have an empty symmetric difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 3050 bins of very similar books.\n",
      "We have 6410 asins for products that have at least one similar product.\n"
     ]
    }
   ],
   "source": [
    "book_only_candidates['title'] = book_only_candidates['title'].apply(clean_title)\n",
    "very_similars = get_similar_titles(book_only_candidates,similars)\n",
    "print(\"We have found {} bins of very similar books.\".format(len(very_similars.keys())))\n",
    "very_similar_ASIN = list(very_similars.keys())\n",
    "very_similar_ASIN += [item for sublist in list(very_similars.values()) for item in sublist]\n",
    "print(\"We have {} asins for products that have at least one similar product.\".format(len(very_similar_ASIN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ```observational_print``` argument of ```get_similar_titles``` one can observe how our function is quite conservative and why:\n",
    "\n",
    "An example where the titles were not matched, providing there difference:\n",
    "```\n",
    "'college mathematics for business economics life sciences and social sciences 12th edition barnett'\n",
    "'finite mathematics for business economics life sciences and social sciences 12th edition barnett'\n",
    "\t --->'college','finite'\n",
    "```\n",
    "and another one\n",
    "```\n",
    "'mastering the nikon d700'\n",
    "'mastering the nikon d600'\n",
    "\t --->'d700','d600\n",
    "```\n",
    "\n",
    "We can see what titles are going to be matched : \n",
    "*What is interesting here is that the last title contains a repetition of the title inside parenthesis, after the cleaning, we will have removed the parenthesis and will check if the symmetric difference in terms of set of words is empty.*\n",
    "```\n",
    "\"Jeff Herman's Guide to Book Publishers, Editors & Literary Agents: Who They Are! What They Want! and How to Win Them Over!\",\n",
    "\"Jeff Herman's Guide to Book Publishers, Editors and Literary Agents : Who They Are! What They Want! How to Win Them Over!\",\n",
    "\"Jeff Herman's Guide to Book Publishers, Editors, and Literary Agents: Who They Are! What They Want! How to Win Them Over! (Jeff Herman's Guide to Book Publishers, Editors, & Literary Agents)\"\n",
    "```\n",
    "*or this second example where a ':' has been replace with a ';'*\n",
    "```\n",
    "'Hawaii the Big Island Revealed: The Ultimate Guidebook',\n",
    "'Hawaii The Big Island Revealed; The Ultimate Guidebook',\n",
    "```\n",
    "\n",
    "\n",
    "While we know that we might be missing a lot of titles, we much rather have less title and 0 false positives than to risk having false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data using pickle for further analysis\n",
    "Now that we have obtained the dataset that we wish to use to conduct our analysis it is time to save it locally to avoid long and repetitive operations while we do the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ANALYSIS_DATA_FOLDER = 'analysis_data/'\n",
    "books_df_path = ANALYSIS_DATA_FOLDER + 'books_full'\n",
    "review_books_df_path = ANALYSIS_DATA_FOLDER + 'review_books_full'\n",
    "very_similars_path = ANALYSIS_DATA_FOLDER + 'very_similars_full'\n",
    "\n",
    "paths = [books_df_path,review_books_df_path,very_similars_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving from :\n",
      "\tanalysis_data/books_full\n",
      "\tanalysis_data/review_books_full\n",
      "\tanalysis_data/very_similars_full\n",
      "We have retrieved:\n",
      "\tbooks_df with shape: (6410, 10)\n",
      "\treview_books_df with shape: (361389, 3)\n",
      "\tvery_similars with length: 3050\n"
     ]
    }
   ],
   "source": [
    "retrieved = []\n",
    "# Get the file from the dump\n",
    "if(all(os.path.isfile(p) for p in paths)):\n",
    "    # We check if the files already exist\n",
    "    print(\"Retrieving from :\")\n",
    "    for i,p in enumerate(paths):\n",
    "        print(\"\\t{}\".format(p))\n",
    "        if(i<2):\n",
    "            df = pd.read_pickle(p)\n",
    "            retrieved.append(df)\n",
    "        else:\n",
    "            with open(p, 'rb') as handle:\n",
    "                very_similars = pickle.load(handle)\n",
    "                retrieved.append(very_similars)\n",
    "else:\n",
    "    # Otherwise we create them\n",
    "    book_only_candidates.loc[very_similar_ASIN].to_pickle(ANALYSIS_DATA_FOLDER + 'books_full')\n",
    "    review_books.query('asin in @very_similar_ASIN').to_pickle(ANALYSIS_DATA_FOLDER + 'review_books_full')\n",
    "    with open(ANALYSIS_DATA_FOLDER + 'very_similars_full', 'wb') as handle:\n",
    "        pickle.dump(very_similars, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "books_df,review_books_df,very_similars=retrieved\n",
    "print(\"We have retrieved:\\n\\tbooks_df with shape: {}\\n\\treview_books_df with shape: {}\\n\\tvery_similars with length: {}\".format(books_df.shape,review_books_df.shape,len(very_similars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herding Effect analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
